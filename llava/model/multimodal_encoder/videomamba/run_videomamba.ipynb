{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(Path(\".\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -e causal-conv1d\n",
    "# !pip3 install -e mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "# from videomamba.video_mm.exp_zs.msrvtt.config\n",
    "\n",
    "from models.umt_videomamba import UMT_VIDEOMAMBA\n",
    "from utils.easydict import EasyDict\n",
    "\n",
    "num_frames = 8\n",
    "img_size = 224\n",
    "batch_size = 64\n",
    "max_txt_l = 32\n",
    "\n",
    "# model_pth = \"videomamba_m16_k400_mask_pt_f8_res224.pth\"  # default in all configs\n",
    "# model_pth = \"videomamba_m16_5M_f8_res224.pth\"  # broken pos\n",
    "model_pth = \"videomamba_m16_k400_mask_ft_f8_res224.pth\"\n",
    "\n",
    "config_dict = {\n",
    "    \"num_frames\": num_frames,\n",
    "    \"num_frames_test\": num_frames,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_txt_l\": max_txt_l,\n",
    "    \"inputs\": {\n",
    "        \"image_res\": img_size,\n",
    "        \"video_input\": {\n",
    "            \"num_frames\": num_frames,\n",
    "            \"sample_type\": \"rand\",\n",
    "            \"num_frames_test\": num_frames,\n",
    "            \"sample_type_test\": \"middle\",\n",
    "            \"random_aug\": False\n",
    "        },\n",
    "        \"max_txt_l\": {\n",
    "            \"image\": max_txt_l,\n",
    "            \"video\": max_txt_l\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"image\": batch_size,\n",
    "            \"video\": batch_size\n",
    "        },\n",
    "        \"batch_size_test\": {\n",
    "            \"image\": batch_size,\n",
    "            \"video\": batch_size\n",
    "        }\n",
    "    },\n",
    "    \"text_enc\": \"bert\",\n",
    "    \"model\": {\n",
    "        \"model_cls\": UMT_VIDEOMAMBA,\n",
    "        \"vision_encoder\": {\n",
    "            \"name\": \"videomamba_middle\",\n",
    "            \"img_size\": img_size,\n",
    "            \"patch_size\": 16,\n",
    "            \"depth\": 32,\n",
    "            \"embed_dim\": 576,\n",
    "            \"drop_path_rate\": 0.25,\n",
    "            \"ssm_cfg\": None,\n",
    "            \"norm_epsilon\": 1e-5,\n",
    "            \"fused_add_norm\": True,\n",
    "            \"rms_norm\": True,\n",
    "            \"residual_in_fp32\": True,\n",
    "            \"bimamba\": True,\n",
    "            \"pool_type\": \"cls+avg\",\n",
    "            \"kernel_size\": 1,\n",
    "            \"num_frames\": num_frames,\n",
    "            \"ckpt_num_frame\": 8,\n",
    "            \"use_checkpoint\": False,\n",
    "            \"checkpoint_num\": 0,\n",
    "            \"clip_decoder_embed_dim\": 576,\n",
    "            \"clip_output_dim\": 512,\n",
    "            \"clip_norm_type\": \"l2\",\n",
    "            \"clip_return_layer\": 1,\n",
    "            \"clip_student_return_interval\": 1,\n",
    "            \"pretrained\": model_pth,\n",
    "            \"clip_teacher\": \"none\",\n",
    "            \"clip_img_size\": img_size,\n",
    "            \"clip_return_interval\": 1,\n",
    "            \"video_mask_type\": \"none\",\n",
    "            \"video_mask_ratio\": 0.0,\n",
    "            \"video_double_mask_ratio\": 0.0,\n",
    "            \"image_mask_type\": \"none\",\n",
    "            \"image_mask_ratio\": 0.0,\n",
    "            \"image_double_mask_ratio\": 0.0,\n",
    "            \"keep_temporal\": True\n",
    "        },\n",
    "        \"text_encoder\": {\n",
    "            \"name\": \"bert_base\",\n",
    "            \"pretrained\": \"bert-base-uncased\",\n",
    "            \"config\": \"configs/config_bert.json\",\n",
    "            \"d_model\": 768,\n",
    "            \"fusion_layer\": 9\n",
    "        },\n",
    "        \"multimodal\": {\"enable\": True},\n",
    "        \"embed_dim\": 512,\n",
    "        \"temp\": 0.07\n",
    "    },\n",
    "    \"criterion\": {\n",
    "        \"loss_weight\": {\n",
    "            \"vtc\": 1.0,\n",
    "            \"mlm\": 1.0,\n",
    "            \"vtm\": 1.0,\n",
    "            \"uta\": 0.0\n",
    "        },\n",
    "        \"vtm_hard_neg\": True,\n",
    "        \"mlm_masking_prob\": 0.5,\n",
    "        \"uta_norm_type\": \"l2\",\n",
    "        \"uta_loss_type\": \"l2\"\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"opt\": \"adamW\",\n",
    "        \"lr\": 1e-5,\n",
    "        \"opt_betas\": [0.9, 0.999],\n",
    "        \"weight_decay\": 0.02,\n",
    "        \"max_grad_norm\": -1,\n",
    "        \"different_lr\": {\"enable\": False, \"module_names\": [], \"lr\": 4e-3}\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"sched\": \"cosine\",\n",
    "        \"epochs\": 2,\n",
    "        \"min_lr_multi\": 0.01,\n",
    "        \"warmup_epochs\": 0.2,\n",
    "        \"num_warmup_steps\": 8,\n",
    "        \"num_training_steps\": 1000,\n",
    "    },\n",
    "    \"evaluate\": False,\n",
    "    \"deep_fusion\": False,\n",
    "    \"evaluation\": {\n",
    "        \"eval_frame_ensemble\": \"concat\",\n",
    "        \"eval_x_only\": False,\n",
    "        \"k_test\": 128,\n",
    "        \"eval_offload\": False\n",
    "    },\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"device\": \"cuda\",\n",
    "    \"mode\": \"pt\",\n",
    "    \"output_dir\": None,\n",
    "    \"resume\": False,\n",
    "    \"debug\": False,\n",
    "    \"log_freq\": 1,\n",
    "    \"seed\": 42,\n",
    "    \"zero_shot\": True,\n",
    "    \"save_latest\": False,\n",
    "    \"auto_resume\": False,\n",
    "    \"pretrained_path\": model_pth,\n",
    "    \"distributed\": False,\n",
    "}\n",
    "\n",
    "config = EasyDict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-15 07:58:10--  https://huggingface.co/OpenGVLab/VideoMamba/resolve/main/videomamba_m16_k400_mask_ft_f8_res224.pth\n",
      "Resolving huggingface.co (huggingface.co)... 18.154.227.7, 18.154.227.69, 18.154.227.67, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.154.227.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/60/50/60507f0fe1a4d5abb54b48fe1e032af809ede0f905d09ec0dd581d9a9e9e6b3f/a101883c7ce8794b887469bcbbce7e09c0c6962dfae23d30223f0dd96275272f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27videomamba_m16_k400_mask_ft_f8_res224.pth%3B+filename%3D%22videomamba_m16_k400_mask_ft_f8_res224.pth%22%3B&Expires=1713427090&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzQyNzA5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzYwLzUwLzYwNTA3ZjBmZTFhNGQ1YWJiNTRiNDhmZTFlMDMyYWY4MDllZGUwZjkwNWQwOWVjMGRkNTgxZDlhOWU5ZTZiM2YvYTEwMTg4M2M3Y2U4Nzk0Yjg4NzQ2OWJjYmJjZTdlMDljMGM2OTYyZGZhZTIzZDMwMjIzZjBkZDk2Mjc1MjcyZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=CO969V1c7ZBEFcU6MwHtynA9P2HxWK0xBHQROUayzzyDGD66YGeChwPxRSBAZ6BI0NH1EuFmdYf9tP997Nc5udCgseAd6eaT0dr2INAlcHlJaxu4VFn-49Lanh2iVCMBf7s7Ek7V4kzoG3dn5pQuyq1Z%7E60b-ZjFmOL2URTxid8wHdGYqXX1eUQ1Y4CpbhBiqGfZE2EBDPgh2qyeJpU3Evrb4KOqfZ%7EFPbFZ4vdhoGin6xbDxVaDyAMltUNTTygoBNSm25jAznrAjwMHh3Zzrs5KX5i6CW%7EsuxKpaj80arO9oy0N5JDAk8p-fT4Jvb9JFrlyvSM8hWjcpJZdcpV%7Eog__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-04-15 07:58:10--  https://cdn-lfs-us-1.huggingface.co/repos/60/50/60507f0fe1a4d5abb54b48fe1e032af809ede0f905d09ec0dd581d9a9e9e6b3f/a101883c7ce8794b887469bcbbce7e09c0c6962dfae23d30223f0dd96275272f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27videomamba_m16_k400_mask_ft_f8_res224.pth%3B+filename%3D%22videomamba_m16_k400_mask_ft_f8_res224.pth%22%3B&Expires=1713427090&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzQyNzA5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzYwLzUwLzYwNTA3ZjBmZTFhNGQ1YWJiNTRiNDhmZTFlMDMyYWY4MDllZGUwZjkwNWQwOWVjMGRkNTgxZDlhOWU5ZTZiM2YvYTEwMTg4M2M3Y2U4Nzk0Yjg4NzQ2OWJjYmJjZTdlMDljMGM2OTYyZGZhZTIzZDMwMjIzZjBkZDk2Mjc1MjcyZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=CO969V1c7ZBEFcU6MwHtynA9P2HxWK0xBHQROUayzzyDGD66YGeChwPxRSBAZ6BI0NH1EuFmdYf9tP997Nc5udCgseAd6eaT0dr2INAlcHlJaxu4VFn-49Lanh2iVCMBf7s7Ek7V4kzoG3dn5pQuyq1Z%7E60b-ZjFmOL2URTxid8wHdGYqXX1eUQ1Y4CpbhBiqGfZE2EBDPgh2qyeJpU3Evrb4KOqfZ%7EFPbFZ4vdhoGin6xbDxVaDyAMltUNTTygoBNSm25jAznrAjwMHh3Zzrs5KX5i6CW%7EsuxKpaj80arO9oy0N5JDAk8p-fT4Jvb9JFrlyvSM8hWjcpJZdcpV%7Eog__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.160.46.26, 18.160.46.36, 18.160.46.59, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.160.46.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 295706698 (282M) [application/zip]\n",
      "Saving to: ‘videomamba_m16_k400_mask_ft_f8_res224.pth’\n",
      "\n",
      "videomamba_m16_k400 100%[===================>] 282.01M  58.3MB/s    in 4.8s    \n",
      "\n",
      "2024-04-15 07:58:15 (58.3 MB/s) - ‘videomamba_m16_k400_mask_ft_f8_res224.pth’ saved [295706698/295706698]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://huggingface.co/OpenGVLab/VideoMamba/resolve/main/videomamba_m16_k400_mask_pt_f8_res224.pth\n",
    "# !wget https://huggingface.co/OpenGVLab/VideoMamba/resolve/main/videomamba_m16_5M_f8_res224.pth\n",
    "# !wget https://huggingface.co/OpenGVLab/VideoMamba/resolve/main/videomamba_m16_k400_mask_ft_f8_res224.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e282a868e824535ae815c88a352e7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "# from videomamba.video_mm.tasks.retrieval\n",
    "\n",
    "from tasks.shared_utils import setup_model\n",
    "\n",
    "(\n",
    "    model,\n",
    "    model_without_ddp,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    scaler,\n",
    "    tokenizer,\n",
    "    start_epoch,\n",
    "    global_step,\n",
    ") = setup_model(\n",
    "    config,\n",
    "    model_cls=config.model.model_cls,\n",
    "    has_decoder=False,\n",
    "    pretrain=False,\n",
    "    # find_unused_parameters=True,\n",
    "    find_unused_parameters=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess video\n",
    "# from videomamba.video_mm.dataset.__init__ -> create_dataset()\n",
    "# from videomamba.video_mm.dataset.base_dataset -> ImageVideoBaseDataset.load_and_transform_media_data_video()\n",
    "\n",
    "from torchvision import transforms\n",
    "from dataset.video_utils import read_frames_decord\n",
    "\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "normalize = transforms.Normalize(mean, std)\n",
    "\n",
    "# loaded images and videos are torch.Tensor of torch.uint8 format,\n",
    "# ordered as (T, 1 or 3, H, W) where T=1 for image\n",
    "type_transform = transforms.Lambda(lambda x: x.float().div(255.0))\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            (config.inputs.image_res, config.inputs.image_res),\n",
    "            interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "        ),\n",
    "        type_transform,\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "video_reader = read_frames_decord\n",
    "\n",
    "max_num_frames = -1\n",
    "frames, frame_indices, video_duration = video_reader(\n",
    "    \"/data/vlm_sandbox/videos/lie1.mp4\",\n",
    "    config.inputs.video_input.num_frames,\n",
    "    config.inputs.video_input.sample_type,\n",
    "    max_num_frames=max_num_frames,\n",
    "    client=None,\n",
    "    trimmed30=False,\n",
    ")\n",
    "\n",
    "frames = transform(frames).unsqueeze(0)\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode image\n",
    "# from videomamba.video_mm.tasks.retrieval_utils -> extract_vision_feats()\n",
    "\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "image_feats_all = []\n",
    "pooled_image_feats_all = []\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    image = frames.to(device, non_blocking=True)\n",
    "    image_feat, pooled_image_feat = model.encode_vision(image, test=True)\n",
    "\n",
    "if len(image_feat.shape) == 4:\n",
    "    image_feat = rearrange(image_feat, \"b t l c -> b (t l) c\").contiguous()\n",
    "image_feat = image_feat.unsqueeze(1)  # (bsz, 1, #frm*L, d)\n",
    "\n",
    "if config.evaluation.eval_offload:\n",
    "    image_feats_all.append(image_feat.cpu())\n",
    "    pooled_image_feats_all.append(pooled_image_feat.cpu())\n",
    "else:\n",
    "    image_feats_all.append(image_feat)\n",
    "    pooled_image_feats_all.append(pooled_image_feat)\n",
    "\n",
    "image_feats_all = torch.cat(image_feats_all, dim=0)\n",
    "pooled_image_feats_all = torch.cat(pooled_image_feats_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text\n",
    "# from videomamba.video_mm.tasks.retrieval_utils -> extract_text_feats()\n",
    "\n",
    "text = \"Person behind a table\"\n",
    "text_bs = 256\n",
    "text_feats = []\n",
    "text_atts = []\n",
    "\n",
    "text_input = tokenizer(\n",
    "    text,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_txt_l,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "text_feat = model.encode_text(text_input)[0]\n",
    "text_feats.append(text_feat)\n",
    "text_atts.append(text_input.attention_mask)\n",
    "\n",
    "text_feats = torch.cat(text_feats, dim=0)\n",
    "text_atts = torch.cat(text_atts, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1568, 576])\n",
      "torch.Size([1, 8, 576])\n",
      "torch.Size([1, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "print(image_feats_all.shape)\n",
    "print(pooled_image_feats_all.shape)\n",
    "print(text_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0031070709228515625\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity\n",
    "# from videomamba.video_mm.tasks.retrieval_utils -> evaluation()\n",
    "\n",
    "from models.criterions import get_sim\n",
    "\n",
    "_pooled_image_feats = (\n",
    "    pooled_image_feats_all.to(device, non_blocking=True)\n",
    "    if config.evaluation.eval_offload\n",
    "    else pooled_image_feats_all\n",
    ")\n",
    "with torch.cuda.amp.autocast():\n",
    "    i2t_scores, t2i_scores = get_sim(\n",
    "        model.vision_proj(_pooled_image_feats), model.text_proj(text_feats[:, 0])\n",
    "    )\n",
    "\n",
    "print(float(i2t_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
