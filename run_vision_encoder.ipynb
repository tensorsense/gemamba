{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "sys.path.append(Path(\".\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-18 07:24:02,128] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from llava.model.multimodal_encoder.videomamba.models.backbones.videomamba import build_videomamba\n",
    "from llava.model.multimodal_encoder.videomamba.vision_tower import DEFAULT_VIDEOMAMBA_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_VIDEOMAMBA_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_videomamba(DEFAULT_VIDEOMAMBA_CONFIG.model)\n",
    "from llava.model.multimodal_encoder.videomamba.models.backbones.videomamba.videomamba import (\n",
    "    PretrainVideoMamba,\n",
    ")\n",
    "\n",
    "config = DEFAULT_VIDEOMAMBA_CONFIG.model\n",
    "\n",
    "model = PretrainVideoMamba(\n",
    "    img_size=config.vision_encoder.img_size,\n",
    "    patch_size=config.vision_encoder.patch_size,\n",
    "    depth=config.vision_encoder.depth,\n",
    "    embed_dim=config.vision_encoder.embed_dim,\n",
    "    drop_path_rate=config.vision_encoder.drop_path_rate,\n",
    "    ssm_cfg=config.vision_encoder.ssm_cfg,\n",
    "    norm_epsilon=config.vision_encoder.norm_epsilon,\n",
    "    fused_add_norm=config.vision_encoder.fused_add_norm,\n",
    "    rms_norm=config.vision_encoder.rms_norm,\n",
    "    residual_in_fp32=config.vision_encoder.residual_in_fp32,\n",
    "    bimamba=config.vision_encoder.bimamba,\n",
    "    pool_type=config.vision_encoder.pool_type,\n",
    "    kernel_size=config.vision_encoder.kernel_size,\n",
    "    num_frames=config.vision_encoder.num_frames,\n",
    "    use_checkpoint=config.vision_encoder.use_checkpoint,\n",
    "    checkpoint_num=config.vision_encoder.checkpoint_num,\n",
    "    clip_decoder_embed_dim=config.vision_encoder.clip_decoder_embed_dim,\n",
    "    clip_output_dim=config.vision_encoder.clip_output_dim,\n",
    "    clip_return_layer=config.vision_encoder.clip_return_layer,\n",
    "    clip_student_return_interval=config.vision_encoder.clip_student_return_interval,\n",
    "    add_pool_norm=True,  # TO GET POOLED FEATURES\n",
    ")\n",
    "# model.default_cfg = _cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/OpenGVLab/VideoMamba/resolve/main/videomamba_m16_5M_f8_res224.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(DEFAULT_VIDEOMAMBA_CONFIG.model.vision_encoder.pretrained, map_location=\"cpu\")\n",
    "checkpoint = torch.load(\"videomamba_m16_5M_f8_res224.pth\", map_location=\"cpu\")\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoMambaEncoder(torch.nn.Module):\n",
    "    def __init__(self, vision_encoder, vision_proj):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.vision_proj = vision_proj\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.vision_encoder(x)\n",
    "        x = self.vision_proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    if \"vision_encoder\" in k or \"vision_proj\" in k:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"model\" in checkpoint.keys():\n",
    "#     state_dict = checkpoint[\"model\"]\n",
    "# else:\n",
    "#     state_dict = checkpoint\n",
    "\n",
    "# msg = model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder = PretrainVideoMamba(\n",
    "    img_size=config.vision_encoder.img_size,\n",
    "    patch_size=config.vision_encoder.patch_size,\n",
    "    depth=config.vision_encoder.depth,\n",
    "    embed_dim=config.vision_encoder.embed_dim,\n",
    "    drop_path_rate=config.vision_encoder.drop_path_rate,\n",
    "    ssm_cfg=config.vision_encoder.ssm_cfg,\n",
    "    norm_epsilon=config.vision_encoder.norm_epsilon,\n",
    "    fused_add_norm=config.vision_encoder.fused_add_norm,\n",
    "    rms_norm=config.vision_encoder.rms_norm,\n",
    "    residual_in_fp32=config.vision_encoder.residual_in_fp32,\n",
    "    bimamba=config.vision_encoder.bimamba,\n",
    "    pool_type=config.vision_encoder.pool_type,\n",
    "    kernel_size=config.vision_encoder.kernel_size,\n",
    "    num_frames=config.vision_encoder.num_frames,\n",
    "    use_checkpoint=config.vision_encoder.use_checkpoint,\n",
    "    checkpoint_num=config.vision_encoder.checkpoint_num,\n",
    "    clip_decoder_embed_dim=config.vision_encoder.clip_decoder_embed_dim,\n",
    "    clip_output_dim=config.vision_encoder.clip_output_dim,\n",
    "    clip_return_layer=config.vision_encoder.clip_return_layer,\n",
    "    clip_student_return_interval=config.vision_encoder.clip_student_return_interval,\n",
    "    add_pool_norm=True,  # TO GET POOLED FEATURES\n",
    ")\n",
    "\n",
    "\n",
    "# vision_width = config.model.vision_encoder.embed_dim\n",
    "# text_width = config.model.text_encoder.d_model\n",
    "#     self.embed_dim = config.model.embed_dim\n",
    "\n",
    "vision_proj = torch.nn.Linear(config.vision_encoder.embed_dim, config.embed_dim)\n",
    "\n",
    "model = VideoMambaEncoder(vision_encoder, vision_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(new_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_embeds, pooled_vision_embeds, _ = self.vision_encoder(\n",
    "    image, None, use_image, keep_temporal,\n",
    ")\n",
    "\n",
    "vision_proj = self.vision_proj(pooled_vision_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "\n",
    "vision_tower = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\", device_map=\"cuda\")\n",
    "image_forward_outs = vision_tower()\n",
    "\n",
    "\n",
    "select_layer = -2\n",
    "select_feature = \"patch\"\n",
    "\n",
    "#(batch_size, sequence_length, hidden_size).\n",
    "\n",
    "image_features = image_forward_outs.hidden_states[select_layer]\n",
    "if select_feature == 'patch':\n",
    "    image_features = image_features[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[-2][:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
