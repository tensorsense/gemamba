{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(Path(\".\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load `VideoMambaVideoEncoder` from checkpoint.\n",
    "2. Load and run forward on `VideoMambaVisionModel`.\n",
    "3. Load `VideoMambaTextModel` from checkpoint.\n",
    "4. Assemble and run `VideoMambaModel`.\n",
    "5. Create a processor class to match `VideoMambaModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load VideoMambaVideoEncoder with weights\n",
    "\n",
    "from llava.model.multimodal_encoder.videomamba2.modeling_videomamba import VideoMambaVideoEncoder, VideoMambaVisionModel, VideoMambaTextModel\n",
    "from llava.model.multimodal_encoder.videomamba2.configuration_videomamba import VideoMambaVisionConfig, VideoMambaTextConfig\n",
    "from llava.model.multimodal_encoder.videomamba2.video_processing_videomamba import VideoMambaVideoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VideoMambaVisionConfig()\n",
    "model = VideoMambaVideoEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/workspaces/gemamba/videomamba_m16_25M_f8_res224.pth\", map_location=\"cpu\")\n",
    "\n",
    "new_state_dict = {}\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    # if \"vision_encoder\" in k or \"vision_proj\" in k:\n",
    "    if \"vision_encoder\" in k:\n",
    "        new_key = k[len(\"vision_encoder.\"):]\n",
    "        # print(new_key)\n",
    "        new_state_dict[new_key] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = model.load_state_dict(new_state_dict, strict=True)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = VideoMambaVideoProcessor()\n",
    "\n",
    "video_paths = [\n",
    "    \"videos_zero_shot/birds.mp4\",\n",
    "    \"videos_zero_shot/fish.mp4\",\n",
    "    \"videos_zero_shot/human.mp4\",\n",
    "    \"videos_zero_shot/swamp.mp4\",\n",
    "]\n",
    "\n",
    "videos = processor(video_paths, return_tensors=\"pt\")[\"pixel_values\"].to(\"cuda\")\n",
    "\n",
    "\n",
    "# b, _, t, _, _ = videos.shape\n",
    "videos.shape\n",
    "model = model.to(\"cuda\")\n",
    "model(videos)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build VideoMambaVisionModel\n",
    "\n",
    "vision_model = VideoMambaVisionModel(config)\n",
    "vision_model.vision_model.load_state_dict(new_state_dict, strict=True)\n",
    "vision_model.vision_model = vision_model.vision_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model(videos).vision_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load a text model\n",
    "\n",
    "text_config = VideoMambaTextConfig.from_json_file(\"llava/model/multimodal_encoder/videomamba/configs/config_bert.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = VideoMambaTextModel.from_pretrained(\"bert-base-uncased\", config=text_config, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0390,  0.2326, -0.2814,  ..., -0.2142, -0.0064,  0.1941],\n",
       "         [ 0.3832, -0.3164, -0.2389,  ..., -0.3462,  0.1316,  0.3012],\n",
       "         [ 0.2595,  0.0545, -0.4032,  ..., -0.3395, -0.2595, -0.1076],\n",
       "         [ 0.8996,  0.1475, -0.4212,  ..., -0.0248, -0.6638, -0.3487]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# text_model(tokenizer(\"some text\"))\n",
    "text_model(**tokenizer(\"some text\", return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create VideoMambaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create VideoMambaProcessor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
