{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "sys.path.append(Path(\".\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-22 07:04:01,655] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llava.model.multimodal_encoder.videomamba.vision_tower import (\n",
    "    DEFAULT_VIDEOMAMBA_CONFIG,\n",
    ")\n",
    "from llava.model.multimodal_encoder.videomamba.models.backbones.videomamba.videomamba import (\n",
    "    PretrainVideoMamba,\n",
    ")\n",
    "from llava.model.multimodal_encoder.videomamba.models.backbones.bert.builder import (\n",
    "    build_bert,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"videomamba_m16_5M_f8_res224.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.multimodal_encoder.videomamba.models.criterions import get_sim\n",
    "\n",
    "\n",
    "class VideoMambaEncoder(torch.nn.Module):\n",
    "    def __init__(self, vision_encoder, vision_proj, text_encoder, text_proj):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.vision_proj = vision_proj\n",
    "\n",
    "        self.text_encoder = text_encoder\n",
    "        self.text_proj = text_proj\n",
    "\n",
    "    def forward(self, video_input, text_input):\n",
    "        vision_feats, pooled_vision_feats, _ = self.vision_encoder(video_input)\n",
    "        # vision_feats = self.vision_proj(pooled_vision_feats)\n",
    "\n",
    "        text_output = self.text_encoder(\n",
    "            text_input.input_ids,\n",
    "            attention_mask=text_input.attention_mask,\n",
    "            return_dict=True,\n",
    "            mode=\"text\",\n",
    "        )\n",
    "\n",
    "        # text_feats = text_output.last_hidden_state\n",
    "\n",
    "        text_embeds = text_output.last_hidden_state\n",
    "        pooled_text_embeds = text_embeds[:, 0]\n",
    "\n",
    "        i2t_scores, t2i_scores = get_sim(\n",
    "            self.vision_proj(pooled_vision_feats), self.text_proj(pooled_text_embeds)\n",
    "        )\n",
    "\n",
    "        return i2t_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    if \"vision_encoder\" in k or \"vision_proj\" in k or \"text_encoder\" in k or \"text_proj\" in k:\n",
    "        if \"bert.\" in k:\n",
    "            k = k.replace(\"bert.\", \"\")\n",
    "        if \"cls.predictions\" in k:\n",
    "            continue\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_VIDEOMAMBA_CONFIG\n",
    "\n",
    "vision_encoder = PretrainVideoMamba(\n",
    "    img_size=config.model.vision_encoder.img_size,\n",
    "    patch_size=config.model.vision_encoder.patch_size,\n",
    "    depth=config.model.vision_encoder.depth,\n",
    "    embed_dim=config.model.vision_encoder.embed_dim,\n",
    "    drop_path_rate=config.model.vision_encoder.drop_path_rate,\n",
    "    ssm_cfg=config.model.vision_encoder.ssm_cfg,\n",
    "    norm_epsilon=config.model.vision_encoder.norm_epsilon,\n",
    "    fused_add_norm=config.model.vision_encoder.fused_add_norm,\n",
    "    rms_norm=config.model.vision_encoder.rms_norm,\n",
    "    residual_in_fp32=config.model.vision_encoder.residual_in_fp32,\n",
    "    bimamba=config.model.vision_encoder.bimamba,\n",
    "    pool_type=config.model.vision_encoder.pool_type,\n",
    "    kernel_size=config.model.vision_encoder.kernel_size,\n",
    "    num_frames=config.model.vision_encoder.num_frames,\n",
    "    use_checkpoint=config.model.vision_encoder.use_checkpoint,\n",
    "    checkpoint_num=config.model.vision_encoder.checkpoint_num,\n",
    "    clip_decoder_embed_dim=config.model.vision_encoder.clip_decoder_embed_dim,\n",
    "    clip_output_dim=config.model.vision_encoder.clip_output_dim,\n",
    "    clip_return_layer=config.model.vision_encoder.clip_return_layer,\n",
    "    clip_student_return_interval=config.model.vision_encoder.clip_student_return_interval,\n",
    "    add_pool_norm=True,  # TO GET POOLED FEATURES\n",
    ")\n",
    "\n",
    "\n",
    "vision_proj = torch.nn.Linear(config.model.vision_encoder.embed_dim, config.model.embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = config.model.text_encoder.name\n",
    "\n",
    "text_encoder = build_bert(\n",
    "    config.model,\n",
    "    False,\n",
    "    config.gradient_checkpointing,\n",
    ")\n",
    "\n",
    "text_proj = torch.nn.Linear(config.model.text_encoder.d_model, config.model.embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoMambaEncoder(vision_encoder, vision_proj, text_encoder, text_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess video\n",
    "# from videomamba.video_mm.dataset.__init__ -> create_dataset()\n",
    "# from videomamba.video_mm.dataset.base_dataset -> ImageVideoBaseDataset.load_and_transform_media_data_video()\n",
    "\n",
    "from llava.model.multimodal_encoder.videomamba.hf_parts.processing_videomamba import VideoMambaVideoProcessor\n",
    "from llava.model.multimodal_encoder.videomamba.models.backbones.bert.tokenization_bert import (\n",
    "    BertTokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config.model.text_encoder.pretrained)\n",
    "processor = VideoMambaVideoProcessor(config, tokenizer=tokenizer)\n",
    "\n",
    "video = \"/data/vlm_sandbox/videos/lie1.mp4\"\n",
    "text = \"Person behind a table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = processor(images=video, text=text, return_tensors=\"pt\")\n",
    "\n",
    "# for key, tensor in inputs.items():\n",
    "#     inputs[key] = tensor.to(\"cuda\")\n",
    "\n",
    "# model.to(\"cuda\")\n",
    "# inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs[\"pixel_values\"].device)\n",
    "# print(inputs[\"input_ids\"].device)\n",
    "# print(inputs[\"token_type_ids\"].device)\n",
    "# print(inputs[\"attention_mask\"].device)\n",
    "\n",
    "# inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim = model(inputs[\"pixel_values\"], inputs)\n",
    "# sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "videos_zero_shot/birds.mp4 :: birds = 0.46033063530921936, fish = 0.3552638292312622, human = 0.20799045264720917, swamp = 0.3916945159435272\n",
      "videos_zero_shot/fish.mp4 :: birds = 0.27160006761550903, fish = 0.4055670499801636, human = 0.20728805661201477, swamp = 0.13486051559448242\n",
      "videos_zero_shot/human.mp4 :: birds = 0.10954683274030685, fish = 0.08383047580718994, human = 0.2469262033700943, swamp = -0.02614482119679451\n",
      "videos_zero_shot/swamp.mp4 :: birds = 0.28155460953712463, fish = 0.280953973531723, human = 0.15472877025604248, swamp = 0.4352746605873108\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "sample_texts = [\"birds\", \"fish\", \"human\", \"swamp\"]\n",
    "sample_videos = [\n",
    "    \"videos_zero_shot/birds.mp4\",\n",
    "    \"videos_zero_shot/fish.mp4\",\n",
    "    \"videos_zero_shot/human.mp4\",\n",
    "    \"videos_zero_shot/swamp.mp4\",\n",
    "]\n",
    "\n",
    "res = defaultdict(list)\n",
    "\n",
    "for video in sample_videos:\n",
    "    for text in sample_texts:\n",
    "        inputs = processor(images=video, text=text, return_tensors=\"pt\")\n",
    "\n",
    "        for key, tensor in inputs.items():\n",
    "            inputs[key] = tensor.to(\"cuda\")\n",
    "\n",
    "        sim = model(inputs[\"pixel_values\"], inputs)[0][0]\n",
    "        res[video].append(sim)\n",
    "\n",
    "for video, sims in res.items():\n",
    "    result_str = \", \".join([f\"{label} = {prob}\" for label, prob in zip(sample_texts, sims)])\n",
    "    print(f\"{video} :: {result_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
